{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset opus_ubuntu (/home/is/koki-tan/.cache/huggingface/datasets/opus_ubuntu/de-en-lang1=de,lang2=en/0.0.0/7ac83b46edf6d0b6ff96bc86d5aadfb8b877c2f136a94af490988c442d3814b8)\n",
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset opus_gnome (/home/is/koki-tan/.cache/huggingface/datasets/opus_gnome/de-en-lang1=de,lang2=en/0.0.0/c00e5dfb1b3b508d7898e160feee1d391e67a3651a06570b45d54ab6a8886217)\n",
      "Reusing dataset opus_openoffice (/home/is/koki-tan/.cache/huggingface/datasets/opus_openoffice/de-en_GB/1.0.0/e891f281b0d9d5d57027b62c759ddc0826ecb289101e88b0ae004c5fe07162ca)\n",
      "Using custom data configuration de-en-lang1=de,lang2=en\n",
      "Reusing dataset kde4 (/home/is/koki-tan/.cache/huggingface/datasets/kde4/de-en-lang1=de,lang2=en/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419aa9cd7b204683b645d1b6c70c8249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13245 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6747b2b5a1424658b8d8da9b7ec81aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28439 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ed1730bccc46ad826d18b0fb7b7689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77052 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcf5ba0fa3843a4a53bc5e84e8af7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224035 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eed14acae2948c8ae39b774aa6eb774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342771 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ca40a164134c3e9957218a5a7543fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=7142), Label(value='0 / 7142'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780f6f6291f24f2db62276ad246939cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5f5c19b4d041a280ddfca8b6c3de12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e67a4cd90e47fe8e793be5d49b0547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas() \n",
    "\n",
    "_train_ubuntu=load_dataset(\"opus_ubuntu\", lang1=\"de\", lang2=\"en\",split='train')  \n",
    "_train_gnome =load_dataset(\"opus_gnome\", lang1=\"de\", lang2=\"en\",split='train') \n",
    "_train_office=load_dataset(\"opus_openoffice\", \"de-en_GB\",split='train')         \n",
    "_train_kde   =load_dataset(\"kde4\", lang1=\"de\", lang2=\"en\",split='train')     \n",
    "\n",
    "from ast import arg\n",
    "\n",
    "idx=-1\n",
    "def custom_column(example,src_code,tgt_code,ds):\n",
    "    global idx\n",
    "    idx+=1\n",
    "    is_Start=False\n",
    "    is_End=False\n",
    "    if(ds[0]==example):\n",
    "        is_Start=True\n",
    "\n",
    "    if(ds[-1]==example):\n",
    "        is_End=True\n",
    "    if(tgt_code=='en_GB'):\n",
    "        return {\"idx\":idx,'en':example['translation'][tgt_code],src_code:example['translation'][src_code],\"is_Start\":is_Start,\"is_End\":is_End,'label':1}\n",
    "    else:\n",
    "        return {\"idx\":idx,tgt_code:example['translation'][tgt_code],src_code:example['translation'][src_code],\"is_Start\":is_Start,\"is_End\":is_End,'label':1}\n",
    "\n",
    "def func_train_ubuntu(example):\n",
    "    return custom_column(example,src_code='de',tgt_code='en',ds=_train_ubuntu)\n",
    "def func_train_gnome (example):\n",
    "    return custom_column(example,src_code='de',tgt_code='en',ds=_train_gnome )\n",
    "def func_train_office(example):\n",
    "    return custom_column(example,src_code='de',tgt_code='en_GB',ds=_train_office)\n",
    "def func_train_kde   (example):\n",
    "    return custom_column(example,src_code='de',tgt_code='en',ds=_train_kde   )\n",
    "\n",
    "train_ubuntu=_train_ubuntu .map(func_train_ubuntu).remove_columns([\"translation\"])\n",
    "train_gnome =_train_gnome  .map(func_train_gnome ).remove_columns([\"translation\"])\n",
    "train_office=_train_office .map(func_train_office).remove_columns([\"translation\"])\n",
    "train_kde   =_train_kde    .map(func_train_kde   ).remove_columns([\"translation\"])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "def merge_and_tokenize_function(x):\n",
    "    en_ids=tokenizer(x[\"en\"],add_special_tokens=False ,padding=\"max_length\", max_length=256,truncation=True)['input_ids']\n",
    "    de_ids=tokenizer(x[\"de\"],add_special_tokens=False, padding=\"max_length\", max_length=256,truncation=True)['input_ids']\n",
    "\n",
    "    return {'id':x['id'],'idx':x['idx'],'en':x['en'],'de':x['de'],'en_ids':en_ids,'de_ids':de_ids,'is_Start':x['is_Start'],'is_End':x['is_End'],'label':1}\n",
    "\n",
    "dataset_positive_train = concatenate_datasets([train_ubuntu,train_gnome,train_office,train_kde]).map(merge_and_tokenize_function)\n",
    "\n",
    "pd_dataset_positive_train=pd.DataFrame(dataset_positive_train)\n",
    "\n",
    "pd_dataset_negative_train=pd_dataset_positive_train\n",
    "\n",
    "pd_dataset_negative_train['de_next']=pd_dataset_positive_train['de_ids'].shift()\n",
    "pd_dataset_negative_train['de_nnext']=pd_dataset_positive_train['de_ids'].shift(2)\n",
    "pd_dataset_negative_train['de_prev']=pd_dataset_positive_train['de_ids'].shift(-1)\n",
    "pd_dataset_negative_train['de_pprev']=pd_dataset_positive_train['de_ids'].shift(-2)\n",
    "\n",
    "pd_dataset_negative_train['next_is_End']=pd_dataset_positive_train['is_End'].shift()\n",
    "pd_dataset_negative_train['prev_is_Start']=pd_dataset_positive_train['is_Start'].shift(-1)\n",
    "\n",
    "\n",
    "def swap(sentence,num,z_index):\n",
    "    if num%2==1:\n",
    "        num-=1\n",
    "    indexes=list(range(z_index))\n",
    "    random.shuffle(indexes)\n",
    "    for i in range(0,num-1,2):\n",
    "        sentence[indexes[i]],sentence[indexes[i+1]]=sentence[indexes[i+1]],sentence[indexes[i]]\n",
    "    return sentence\n",
    "\n",
    "def sampler(sentence):\n",
    "    idx=sentence['idx']\n",
    "    src=sentence['en_ids']\n",
    "    tgt=sentence['de_ids']\n",
    "    n=random.random()\n",
    "    #dt=time.time()\n",
    "    p=None\n",
    "    if(n>2/3):\n",
    "        p=1\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Randomly select a target sentence from its adjacent sentences within a window size of k (where k = 2 in our experiments).\n",
    "        \"\"\"\n",
    "        tgt=random.choice([sentence['de_pprev'],sentence['de_prev'],sentence['de_next'],sentence['de_nnext']])\n",
    "        if(sentence[\"is_Start\"]):\n",
    "            tgt=random.choice([sentence['de_next'],sentence['de_nnext']])\n",
    "        elif(sentence[\"is_End\"]):\n",
    "            tgt=random.choice([sentence['de_prev'],sentence['de_pprev']])\n",
    "        elif(sentence['de_pprev']==None or sentence['prev_is_Start']==True):\n",
    "            tgt=random.choice([sentence['de_prev'],sentence['de_next'],sentence['de_nnext']])\n",
    "        elif(sentence['de_nnext']==None or sentence['next_is_End']==True):\n",
    "            tgt=random.choice([sentence['de_pprev'],sentence['de_prev'],sentence['de_next']])\n",
    "        \n",
    "    elif(n>1/3):\n",
    "        p=2\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Randomly truncate 30%-70% of the source or target sentence.\n",
    "        \"\"\"\n",
    "        u=random.random()\n",
    "        r=random.uniform(0.3,0.7)\n",
    "        if(u<0.5):\n",
    "            zindex=len(sentence['en_ids'])-1\n",
    "            if 0 in sentence['en_ids']:\n",
    "                zindex=sentence['en_ids'].index(0)\n",
    "            src=sentence['en_ids'][:int(len(sentence['en_ids'][:zindex])*r)]+[0]*(len(sentence['en_ids'])-int(len(sentence['en_ids'][:zindex])*r))\n",
    "        else:\n",
    "            zindex=len(sentence['de_ids'])-1\n",
    "            if 0 in sentence['de_ids']:\n",
    "                zindex=sentence['de_ids'].index(0)\n",
    "            tgt=sentence['de_ids'][:int(len(sentence['de_ids'][:zindex])*r)]+[0]*(len(sentence['de_ids'])-int(len(sentence['de_ids'][:zindex])*r))\n",
    "    else:\n",
    "        p=3\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Swap  the  order  of  30%-70%  words  of  the source or target sentence.\n",
    "        \"\"\"\n",
    "        u=random.random()\n",
    "        r=random.uniform(0.3,0.7)\n",
    "        if(u<0.5):\n",
    "            zindex=len(sentence['en_ids'])-1\n",
    "            if 0 in sentence['en_ids']:\n",
    "                zindex=sentence['en_ids'].index(0)\n",
    "            swapnum=int(zindex*r)\n",
    "            src=swap(sentence['en_ids'],swapnum,zindex)\n",
    "        else:\n",
    "            zindex=len(sentence['de_ids'])-1\n",
    "            if 0 in sentence['de_ids']:\n",
    "                zindex=sentence['de_ids'].index(0)\n",
    "            swapnum=int(zindex*r)\n",
    "            tgt=swap(sentence['de_ids'],swapnum,zindex)\n",
    "    #print((time.time()-dt)*1000)\n",
    "    return src,tgt #en<sep>tgt\n",
    "\n",
    "def negative_creater(sentence): \n",
    "    sentence['en_ids'],sentence['de_ids']=sampler(sentence)\n",
    "    sentence['label']=0\n",
    "    return  sentence\n",
    "\n",
    "def func(e):\n",
    "    return negative_creater(e)\n",
    "\n",
    "pd_dataset_negative_train=pd_dataset_negative_train.parallel_apply(func,axis=1)\n",
    "\n",
    "pd_data=pd.concat([pd_dataset_positive_train.loc[:,['en_ids','de_ids','label']],pd_dataset_negative_train.loc[:,['en_ids','de_ids','label']]]).dropna()\n",
    "# negativeとpositiveの結合\n",
    "data= Dataset.from_pandas(pd_data)\n",
    "data.remove_columns(['__index_level_0__']).push_to_hub('ahclab/acceptability_filtering_data_en-de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb0ea3c51ce178d2de91c898de2b7c91e2177c45f33ebf149a753b8454849142"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
