{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 48 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-zh_CN-lang1=en,lang2=zh_CN\n",
      "Reusing dataset opus_ubuntu (/home/is/koki-tan/.cache/huggingface/datasets/opus_ubuntu/en-zh_CN-lang1=en,lang2=zh_CN/0.0.0/7ac83b46edf6d0b6ff96bc86d5aadfb8b877c2f136a94af490988c442d3814b8)\n",
      "Using custom data configuration en-zh_CN-lang1=en,lang2=zh_CN\n",
      "Reusing dataset opus_gnome (/home/is/koki-tan/.cache/huggingface/datasets/opus_gnome/en-zh_CN-lang1=en,lang2=zh_CN/0.0.0/c00e5dfb1b3b508d7898e160feee1d391e67a3651a06570b45d54ab6a8886217)\n",
      "Reusing dataset opus_openoffice (/home/is/koki-tan/.cache/huggingface/datasets/opus_openoffice/en_GB-zh_CN/1.0.0/e891f281b0d9d5d57027b62c759ddc0826ecb289101e88b0ae004c5fe07162ca)\n",
      "Using custom data configuration en-zh_CN-lang1=en,lang2=zh_CN\n",
      "Reusing dataset kde4 (/home/is/koki-tan/.cache/huggingface/datasets/kde4/en-zh_CN-lang1=en,lang2=zh_CN/0.0.0/243129fb2398d5b0b4f7f6831ab27ad84774b7ce374cf10f60f6e1ff331648ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3353431ef57e403886ae4f8a0e8278fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6865 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb972552e804112b4d53b3808dd687b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c95008b1a724301a9e8ede3fe521d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69400 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9595808ec6a8499594b45f55305eb430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139666 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6346ee1cc0c34f41b6787c3ccc00079f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216009 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5622088598854ba2a1ea3ecf59b272ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4501), Label(value='0 / 4501'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf81fd4aac8414d8f1fce733750ee67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4a798076c64de8ac156b7a47e61813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1aa81f1fec4bb78731fba8f24091bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,concatenate_datasets,Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "tqdm.pandas() \n",
    "\n",
    "_train_ubuntu=load_dataset(\"opus_ubuntu\", lang1=\"en\", lang2=\"zh_CN\",split='train')  \n",
    "_train_gnome =load_dataset(\"opus_gnome\", lang1=\"en\", lang2=\"zh_CN\",split='train') \n",
    "_train_office=load_dataset(\"opus_openoffice\", \"en_GB-zh_CN\",split='train')         \n",
    "_train_kde   =load_dataset(\"kde4\", lang1=\"en\", lang2=\"zh_CN\",split='train')     \n",
    "\n",
    "from ast import arg\n",
    "\n",
    "idx=-1\n",
    "def custom_column(example,src_code,tgt_code,ds):\n",
    "    global idx\n",
    "    idx+=1\n",
    "    is_Start=False\n",
    "    is_End=False\n",
    "    if(ds[0]==example):\n",
    "        is_Start=True\n",
    "\n",
    "    if(ds[-1]==example):\n",
    "        is_End=True\n",
    "    if(tgt_code=='en_GB'):\n",
    "        return {\"idx\":idx,'en':example['translation'][tgt_code],'zh':example['translation'][src_code],\"is_Start\":is_Start,\"is_End\":is_End,'label':1}\n",
    "    elif(src_code=='zh_CN'):\n",
    "        return {\"idx\":idx,tgt_code:example['translation'][tgt_code],'zh':example['translation'][src_code],\"is_Start\":is_Start,\"is_End\":is_End,'label':1}\n",
    "    else:\n",
    "        return {\"idx\":idx,tgt_code:example['translation'][tgt_code],'zh':example['translation'][src_code],\"is_Start\":is_Start,\"is_End\":is_End,'label':1}\n",
    "\n",
    "def func_train_ubuntu(example):\n",
    "    return custom_column(example,src_code='zh_CN',tgt_code='en',ds=_train_ubuntu)\n",
    "def func_train_gnome (example):\n",
    "    return custom_column(example,src_code='zh_CN',tgt_code='en',ds=_train_gnome )\n",
    "def func_train_office(example):\n",
    "    return custom_column(example,src_code='zh_CN',tgt_code='en_GB',ds=_train_office)\n",
    "def func_train_kde   (example):\n",
    "    return custom_column(example,src_code='zh_CN',tgt_code='en',ds=_train_kde   )\n",
    "\n",
    "train_ubuntu=_train_ubuntu .map(func_train_ubuntu).remove_columns([\"translation\"])\n",
    "train_gnome =_train_gnome  .map(func_train_gnome ).remove_columns([\"translation\"])\n",
    "train_office=_train_office .map(func_train_office).remove_columns([\"translation\"])\n",
    "train_kde   =_train_kde    .map(func_train_kde   ).remove_columns([\"translation\"])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "def merge_and_tokenize_function(x):\n",
    "    en_ids=tokenizer(x[\"en\"],add_special_tokens=False, padding=\"max_length\", max_length=256,truncation=True)['input_ids']\n",
    "    de_ids=tokenizer(x[\"zh\"],add_special_tokens=False, padding=\"max_length\", max_length=256,truncation=True)['input_ids']\n",
    "    return {'id':x['id'],'idx':x['idx'],'en':x['en'],'zh':x['zh'],'en_ids':en_ids,'zh_ids':de_ids,'is_Start':x['is_Start'],'is_End':x['is_End'],'label':1}\n",
    "\n",
    "dataset_positive_train = concatenate_datasets([train_ubuntu,train_gnome,train_office,train_kde]).map(merge_and_tokenize_function)\n",
    "\n",
    "pd_dataset_positive_train=pd.DataFrame(dataset_positive_train)\n",
    "\n",
    "pd_dataset_negative_train=pd_dataset_positive_train\n",
    "\n",
    "pd_dataset_negative_train['zh_next']= pd_dataset_positive_train['zh_ids'].shift()\n",
    "pd_dataset_negative_train['zh_nnext']=pd_dataset_positive_train['zh_ids'].shift(2)\n",
    "pd_dataset_negative_train['zh_prev']= pd_dataset_positive_train['zh_ids'].shift(-1)\n",
    "pd_dataset_negative_train['zh_pprev']=pd_dataset_positive_train['zh_ids'].shift(-2)\n",
    "\n",
    "pd_dataset_negative_train['next_is_End']=pd_dataset_positive_train['is_End'].shift()\n",
    "pd_dataset_negative_train['prev_is_Start']=pd_dataset_positive_train['is_Start'].shift(-1)\n",
    "\n",
    "\n",
    "def swap(sentence,num,z_index):\n",
    "    if num%2==1:\n",
    "        num-=1\n",
    "    indexes=list(range(z_index))\n",
    "    random.shuffle(indexes)\n",
    "    for i in range(0,num-1,2):\n",
    "        sentence[indexes[i]],sentence[indexes[i+1]]=sentence[indexes[i+1]],sentence[indexes[i]]\n",
    "    return sentence\n",
    "\n",
    "def sampler(sentence):\n",
    "    idx=sentence['idx']\n",
    "    src=sentence['en_ids']\n",
    "    tgt=sentence['zh_ids']\n",
    "    n=random.random()\n",
    "    #dt=time.time()\n",
    "    p=None\n",
    "    if(n>2/3):\n",
    "        p=1\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Randomly select a target sentence from its adjacent sentences within a window size of k (where k = 2 in our experiments).\n",
    "        \"\"\"\n",
    "        tgt=random.choice([sentence['zh_pprev'],sentence['zh_prev'],sentence['zh_next'],sentence['zh_nnext']])\n",
    "        if(sentence[\"is_Start\"]):\n",
    "            tgt=random.choice([sentence['zh_next'],sentence['zh_nnext']])\n",
    "        elif(sentence[\"is_End\"]):\n",
    "            tgt=random.choice([sentence['zh_prev'],sentence['zh_pprev']])\n",
    "        elif(sentence['zh_pprev']==None or sentence['prev_is_Start']==True):\n",
    "            tgt=random.choice([sentence['zh_prev'],sentence['zh_next'],sentence['zh_nnext']])\n",
    "        elif(sentence['zh_nnext']==None or sentence['next_is_End']==True):\n",
    "            tgt=random.choice([sentence['zh_pprev'],sentence['zh_prev'],sentence['zh_next']])\n",
    "        \n",
    "    elif(n>1/3):\n",
    "        p=2\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Randomly truncate 30%-70% of the source or target sentence.\n",
    "        \"\"\"\n",
    "        u=random.random()\n",
    "        r=random.uniform(0.3,0.7)\n",
    "        if(u<0.5):\n",
    "            zindex=len(sentence['en_ids'])-1\n",
    "            if 0 in sentence['en_ids']:\n",
    "                zindex=sentence['en_ids'].index(0)\n",
    "            src=sentence['en_ids'][:int(len(sentence['en_ids'][:zindex])*r)]+[0]*(len(sentence['en_ids'])-int(len(sentence['en_ids'][:zindex])*r))\n",
    "        else:\n",
    "            zindex=len(sentence['zh_ids'])-1\n",
    "            if 0 in sentence['zh_ids']:\n",
    "                zindex=sentence['zh_ids'].index(0)\n",
    "            tgt=sentence['zh_ids'][:int(len(sentence['zh_ids'][:zindex])*r)]+[0]*(len(sentence['zh_ids'])-int(len(sentence['zh_ids'][:zindex])*r))\n",
    "    else:\n",
    "        p=3\n",
    "        #print(p)\n",
    "        \"\"\"\n",
    "        Swap  the  order  of  30%-70%  words  of  the source or target sentence.\n",
    "        \"\"\"\n",
    "        u=random.random()\n",
    "        r=random.uniform(0.3,0.7)\n",
    "        if(u<0.5):\n",
    "            zindex=len(sentence['en_ids'])-1\n",
    "            if 0 in sentence['en_ids']:\n",
    "                zindex=sentence['en_ids'].index(0)\n",
    "            swapnum=int(zindex*r)\n",
    "            src=swap(sentence['en_ids'],swapnum,zindex)\n",
    "        else:\n",
    "            zindex=len(sentence['zh_ids'])-1\n",
    "            if 0 in sentence['zh_ids']:\n",
    "                zindex=sentence['zh_ids'].index(0)\n",
    "            swapnum=int(zindex*r)\n",
    "            tgt=swap(sentence['zh_ids'],swapnum,zindex)\n",
    "    #print((time.time()-dt)*1000)\n",
    "    return src,tgt #en<sep>tgt\n",
    "\n",
    "def negative_creater(sentence): \n",
    "    sentence['en_ids'],sentence['zh_ids']=sampler(sentence)\n",
    "    sentence['label']=0\n",
    "    return  sentence\n",
    "\n",
    "def func(e):\n",
    "    return negative_creater(e)\n",
    "\n",
    "pd_dataset_negative_train=pd_dataset_negative_train.parallel_apply(func,axis=1)\n",
    "\n",
    "pd_data=pd.concat([pd_dataset_positive_train.loc[:,['en_ids','zh_ids','label']],pd_dataset_negative_train.loc[:,['en_ids','zh_ids','label']]]).dropna()\n",
    "# negativeとpositiveの結合\n",
    "data= Dataset.from_pandas(pd_data)\n",
    "data.push_to_hub('ahclab/acceptability_filtering_data_en-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb0ea3c51ce178d2de91c898de2b7c91e2177c45f33ebf149a753b8454849142"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
